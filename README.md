# Datasonic Lakehouse Copilot

This project provides a **Data Agent** built on top of a **Fabric Lakehouse**, enabling **natural language querying** for both business and technical users. By combining **SQLAlchemy**, **LangChain Agents**, and the **OpenAI API**, the solution allows users to ask questions in plain English and get meaningful SQL-driven answers from their data.

Two versions of the application are included, with the same core functionality but differing in **UI chat history retention**.

---

## ğŸš€ Features

* **Natural Language to SQL**: Query the Lakehouse without writing SQL manually.
* **LangChain SQL Agent**: Uses LangChainâ€™s agent toolkit to translate queries into SQL.
* **SQLAlchemy Integration**: Provides reliable connections to SQL Server with `pyodbc`.
* **Schema Extraction Tool**: Includes a `schema_extractor.py` utility to generate schema metadata (`JSON`, `Markdown`, and `CSV`).
* **Streamlit UI**: Simple web interface for user interaction.
* **Conversation History**:

  * `SQL_Alchemy_Langchain_Approach.py` â†’ Basic version, no persistent chat history.
  * `sql_alchemy_v2.py` â†’ Enhanced version with retained chat history for contextual conversations.

---

## ğŸ“‚ Repository Structure

```
.
â”œâ”€â”€ SQL_Alchemy_Langchain_Approach.py   # Initial version (no chat history retention)
â”œâ”€â”€ sql_alchemy_v2.py                   # Enhanced UI with chat history
â”œâ”€â”€ schema_extractor.py                 # Utility for schema extraction & documentation
```

---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone <repo-url>
cd <repo-name>
```

### 2. Install Dependencies

Ensure Python 3.9+ is installed.

```bash
pip install -r requirements.txt
```

**Example requirements** (adjust as needed):

```text
sqlalchemy
pyodbc
streamlit
langchain
langchain-openai
python-dotenv
pandas
```

### 3. Configure Environment Variables

Create a `.env` file in the project root:

```env
SERVER=your_sql_server
DATABASE=your_database
CLIENT_ID=your_client_id
CLIENT_SECRET=your_client_secret
TENANT_ID=your_tenant_id
API_KEY=your_openai_api_key
```

---

## â–¶ï¸ Usage

### Run the Lakehouse Copilot

```bash
streamlit run sql_alchemy_v2.py
```

or

```bash
streamlit run SQL_Alchemy_Langchain_Approach.py
```

* Enter a **natural language query** in the UI.
* The agent translates it into SQL, executes it, and returns the result.
* The **v2 version** keeps chat history for context-aware Q\&A.

### Run the Schema Extractor

```bash
python schema_extractor.py
```

This generates:

* `schema.json` â€“ Full schema metadata.
* `schema.md` â€“ Human-readable Markdown report.
* `columns.csv` â€“ Flattened column inventory.

---

## ğŸ“Š Example Queries

* *"Show me the top 10 customers by policy count"*
* *"List claims from last month grouped by broker"*
* *"Get the row count of the customer table"*

---

## ğŸ› ï¸ Tech Stack

* **[SQLAlchemy](https://www.sqlalchemy.org/)** â€“ Database connection and query execution.
* **[LangChain](https://www.langchain.com/)** â€“ LLM orchestration and SQL Agent.
* **[OpenAI API](https://platform.openai.com/)** â€“ Natural language processing.
* **[Streamlit](https://streamlit.io/)** â€“ Lightweight UI.
* **[PyODBC](https://github.com/mkleehammer/pyodbc)** â€“ SQL Server connectivity.

---

## ğŸ”® Future Enhancements

* Add role-based access controls for queries.
* Integrate with other Lakehouse engines (e.g., Delta, Fabric native).
* Expand to support multimodal outputs (charts, tables, summaries).

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

Would you like me to also create a **ready-to-use `requirements.txt`** file from your uploaded code so you donâ€™t have to manually compile dependencies?
